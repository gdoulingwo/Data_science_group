<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 12"/><title>Acr271.tmp</title><link rel="stylesheet" href="DBN_files/DBN.css" type="text/css"/>
</head>
<body><a name="caption1"></a><h2><a name="bookmark0"></a><span class="font11" style="font-weight:bold;">Dynamic Network Models for Forecasting</span></h2>
<p><span class="font8">Paul Dagum<a name="footnote1"></a><sup><a href="#bookmark1">1</a></sup> Adam Galper Eric Horvitz<a name="footnote1"></a><sup><a href="#bookmark1">1</a></sup></span></p>
<p><span class="font3">Section on Medical Informatics Stanford University School of Medicine&nbsp;Stanford, California 94305</span></p><h3><a name="bookmark2"></a><span class="font10" style="font-weight:bold;">Abstract</span></h3>
<p><span class="font3">We have developed a probabilistic forecasting methodology through a synthesis of belief-network models and classical time-series&nbsp;analysis. We present the </span><span class="font8" style="font-style:italic;">dynamic network&nbsp;model</span><span class="font3"> (DNM) and describe methods for constructing, refining, and performing inference&nbsp;with this representation of temporal probabilistic knowledge. The DNM representation&nbsp;extends static belief-net work models to more&nbsp;general dynamic forecasting models by integrating and iteratively refining contemporaneous and time-lagged dependencies. We discuss key concepts in terms of a model for forecasting U.S. car sales in Japan.</span></p><h3><a name="bookmark3"></a><span class="font10" style="font-weight:bold;">1 INTRODUCTION</span></h3>
<p><span class="font3">Temporal projection and forecasting play a critical role in real-world decision making. Consider the problem of forecasting U.S. car sales in Japan six months,&nbsp;one year, and three years from now. Such forecasts&nbsp;typically are crucial in the planning and execution of&nbsp;corporate and political strategies. A U.S. automobile&nbsp;manufacturer will pay close attention to the predicted&nbsp;Japanese demand for U.S. cars, especially when resources must be allocated for further market penetration. Meanwhile, the U.S. government may adjust its&nbsp;economic and foreign policies to reflect the anticipated&nbsp;penetration of American products in previously inaccessible markets.</span></p>
<p><span class="font3">Forecasting models are dominated by uncertainty because salient, observable variables define only a small subset of relevant variables; unmodeled influences can&nbsp;lead to unexpected consequences in a dynamic process.&nbsp;Of course, measurement and other errors in the observations, functional relationships among variables, and&nbsp;missing data introduce additional uncertainty.</span></p>
<p><span class="font3">Statisticians have developed a set of techniques known </span><span class="font8" style="font-style:italic;">as time-series analysis</span><span class="font3"> for forecasting the future values of variables. A </span><span class="font8" style="font-style:italic;">time series</span><span class="font3"> is a set of observations made sequentially over time. Investigators have&nbsp;sought to develop time-series methods for generating&nbsp;or inducing stochastic models, which describe temporal dependencies among successive observations in a&nbsp;time series.</span></p>
<p><span class="font3">A mature set of probabilistic time-series analysis methods has been developed and applied to a wide range of problems [20]. However, classical time-series methodologies do not provide an expressive representation for&nbsp;capturing the probabilistic dependencies and the nonlinearities of real-world processes. Statisticians have&nbsp;wrestled complex problems into relatively simple parameterized models that can be solved with the traditional methods. Until recently, there has been relatively little interaction between the statisticians interested in time-series analysis and computer-scientists&nbsp;studying the representation of uncertain knowledge&nbsp;with belief networks.</span></p>
<p><span class="font3">Figure 1 depicts a simple belief-network representation of the interactions between salient variables relevant to the problem of forecasting U.S. car sales in Japan. Briefly, the model states that Japanese demand for U.S. cars depends on the price of these cars&nbsp;and influences the supply of the cars. The price and&nbsp;supply of the U.S. cars is influenced by the efficiency&nbsp;of U.S. manufacturers. Notice that this model captures purely contemporaneous dependencies; that is,&nbsp;the model represents relationships within a fixed time&nbsp;frame. We refer to this model as the </span><span class="font8" style="font-style:italic;">static network.</span></p>
<p><span class="font3">In this paper, we address the following problem: Given a static belief network, such as the one depicted in&nbsp;Figure 1, how can we make normative forecasts of the&nbsp;future values of variables? We answer this question&nbsp;by providing an expressive forecasting methodology&nbsp;based on the integration of classical time-series analysis with belief-network representation and inference&nbsp;techniques. Our synthesis has two immediate benefits.&nbsp;First, by casting probabilistic time-series analyses as&nbsp;temporal belief-network problems, we can introduce&nbsp;general dependency models that capture richer, and</span></p><img src="DBN_files/DBN-1.jpg" style="width:231pt;height:102pt;"/>
<p><span class="font3">Figure 1: (a) A static network depicting contemporaneous dependencies between economic variables, (b) </span><span class="font7" style="font-variant:small-caps;">carsales</span><span class="font21">：</span><span class="font7"> </span><span class="font3">A DNM for forecasting U.S. car sales in&nbsp;Japan.</span></p>
<p><span class="font3">more realistic, models of dynamic dependencies—as well as the more traditional static or </span><span class="font8" style="font-style:italic;">contemporaneous</span><span class="font3"> belief-network dependencies. Second, we can apply belief-network inference algorithms to the temporal models to do forecasting. The richer models and&nbsp;associated computational methods allow us to move&nbsp;beyond such rigid classical assumptions as linearity&nbsp;in the relationships among variables and normality of&nbsp;their probability distributions.</span></p>
<p><span class="font3">In Sections 2 and 5, we discuss methods for generating a forecasting model known as a </span><span class="font8" style="font-style:italic;">dynamic network model&nbsp;</span><span class="font3">(DNM) from a static network —for example, Figure lb.&nbsp;Section 3 describes how uncontrolled, unknown exogenous influences can render the forecasting model obsolete unless the model is parameterized; an update&nbsp;method described in Section 3.2 can quickly adapt the&nbsp;model to observed trends. In Section 4, we present&nbsp;procedures for forecasting the future value of a model&nbsp;variable, based on knowledge about the time-lagged&nbsp;values of itself and/or of values of related variables.&nbsp;We conclude with a discussion of the advantages of&nbsp;the DNM approach, and by highlighting representation and inference problems that remain to be solved.</span></p><h3><a name="bookmark4"></a><span class="font10" style="font-weight:bold;">2 DYNAMIC NETWORK MODELS</span></h3>
<p><span class="font3">A dynamic model can be constructed from a set of building blocks that capture the instantaneous relationships between domain variables, together with a&nbsp;set of temporal dependencies that capture the dynamic&nbsp;behavior of the domain variables. The building block&nbsp;of a DNM is the traditional, static belief network. We&nbsp;extend the static belief network displayed in Figure 1&nbsp;to a DNM forecasting model by introducing relevant&nbsp;temporal dependencies between representations of the&nbsp;static network at different times. A DNM for reasoning about future U.S. car sales in Japan is depicted&nbsp;in Figure lb. Nodes represent states of the domain&nbsp;variables at different times. In this case, the interval&nbsp;between time points is defined to be one month. We&nbsp;refer to this DNM as the </span><span class="font7">CARSALES </span><span class="font3">model.</span></p>
<p><span class="font3">As highlighted in Figure lb, we distinguish between two types of dependencies in a DNM. </span><span class="font8" style="font-style:italic;">Contempora-</span><span class="font3">neows dependencies refer to arcs between nodes that&nbsp;represent variables within the same time period. </span><span class="font8" style="font-style:italic;">Noncontemporaneous dependencies</span><span class="font3"> refer to arcs between&nbsp;nodes that represent variables at different times. Thus,&nbsp;the dependency between </span><span class="font8" style="font-style:italic;">corporate health</span><span class="font3"> and </span><span class="font8" style="font-style:italic;">supply&nbsp;</span><span class="font3">is contemporaneous, whereas the dependency between&nbsp;this month’s price and next month’s </span><span class="font8" style="font-style:italic;">supply</span><span class="font3"> is non-contemporaenous. The DNM representation allows&nbsp;us to assert and to selectively weaken independence&nbsp;statements about temporal locality by learning noncontemporaneous dependencies and modulating their&nbsp;strength.</span></p>
<p><span class="font3">Thus far, the specification of the structure of the simple DNM has adhered to traditional belief-network methodology. Our point of departure from the established paradigm is in the specification of the conditional probabilities of the model. As time evolves, so&nbsp;do the potential influences of a plethora of unmodeled&nbsp;exogenous forces that affect the state of the system.&nbsp;The structure and conditional probabilities that define&nbsp;a belief network at one time become obsolete with the&nbsp;passage of time. Discrepancies between new observations and the values predicted by the outdated model&nbsp;attest to a model’s inaccuracy and inability to make&nbsp;reliable forecasts. To avoid the progressive deterioration of the model, an intelligent forecasting system&nbsp;must update the conditional probabilities, and also the&nbsp;structure when appropriate, as new evidence arrives.</span></p>
<p><span class="font3">In the </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">model, exogenous influences that may have a significant effect on the Japanese demand&nbsp;for U.S. cars include, for example, new U.S. trade&nbsp;policies or a recent upset in Japanese-U.S. diplomatic&nbsp;relations. In general, these influences are extremely&nbsp;difficult to model adequately, and their inclusion in&nbsp;the model, if done inappropriately, often leads to systematic errors of prediction. Furthermore, important exogenous events may be unknown to the model&nbsp;builder. For example, an undisclosed pending merger-and-acquisition by a Japanese car manufacturer might&nbsp;manifest as a decrease in the corporate health of the&nbsp;U.S. car industry, in spite of the seemingly unchanged&nbsp;economy. An unfortunate presidential blunder during&nbsp;a state dinner, left unpublicized, may also lead to dire&nbsp;forecasts of model variables.</span></p>
<p><span class="font3">Such sensitivity to unmodeled variables highlights the value of developing a means for updating specific contemporaneous and noncontemporaneous relations that&nbsp;are assessed from a domain expert, as time-series&nbsp;data becomes available. Any probabilistic model that&nbsp;claims to forecast must possess a method of adaptively&nbsp;integrating historical information with current estimates of domain variables. In Section 3.2, we discuss&nbsp;how this requirement imposes the principle of maximum likelihood for the update of conditional probabilities.</span></p><h3><a name="bookmark5"></a><span class="font10" style="font-weight:bold;">3 BUILDING AND REFINING A&nbsp;DNM</span></h3>
<p><span class="font3">To build a DNM, an expert specifies the key contemporaneous variables and dependencies. Then, the expert specifies key temporal dependencies among the variables, drawing dependencies from earlier states of&nbsp;the world to variables in the contemporaneous belief&nbsp;network.</span></p>
<p><span class="font3">Methods for dynamically updating a model specification </span><span class="font8" style="font-style:italic;">as</span><span class="font3"> new evidence becomes available are critical to techniques for forecasting the future states of complex systems. We simplify the task of updating&nbsp;the DNM model specification by the assumption that&nbsp;the model structure—that is, the set of contemporaneous and noncontemporaneous dependencies—remains&nbsp;invariant over time, and only the set of conditional&nbsp;probabilities need to be updated as time evolves. In&nbsp;other words, we assume that, although variations in&nbsp;unmodeled exogenous forces will inevitably affect the&nbsp;strength of the dependencies, changes in the exogenous&nbsp;environment of the model do not introduce new dependencies or nullify existing dependencies. For many&nbsp;systems, this assumption is valid. An example that&nbsp;invalidates the assumption, in the context of a simple supply-demand model, occurs when an essential&nbsp;commodity—that is, one for which demand is virtually fixed—is supplied by a monopolistic power. Monopolistic control of a commodity arises from a cartel&nbsp;between commercial enterprises designed to fix prices&nbsp;regardless of the demand.</span></p>
<p><span class="font3">To design a DNM that dynamically updates conditional probabilities when new evidence becomes available, we need to address two crucial problems. First, we must specify conditional probabilities that are&nbsp;amenable to incremental adjustments which reflect&nbsp;changes in exogenous forces—the </span><span class="font8" style="font-style:italic;">assessment task.&nbsp;</span><span class="font3">Then, we must specify how, given new observations,&nbsp;we adjust the conditional probabilities without introducing biases and with minimization of the expected&nbsp;error in the forecasts—the </span><span class="font8" style="font-style:italic;">update task.</span></p>
<p><span class="font8">3.1 Assessing DNM Probabilities</span></p>
<p><span class="font3">To assess the conditional probabilities, we employ an approach that draws on the best elements of expert-assessment and parameter-estimation techniques. The&nbsp;parametric decomposition is chosen by an expert in&nbsp;anticipation of the key components of the conditional&nbsp;probabilities that need to be adjusted to reflect sudden changes in the unmodeled exogenous environment.&nbsp;The expert strives to attain a balance between an expressive and a parsimonious parameterization. The&nbsp;use of a conditional probability table for each node,&nbsp;with each table entry treated as an updatable parameter, is an expressive parameterization, yet complex&nbsp;parameterization. The complexity of the assessment&nbsp;procedure virtually precludes making unbiased adjustments of the conditional probabilities. In Section 5&nbsp;we discuss two parameterization models for the conditional probabilities. These models are motivated by&nbsp;the methods used currently by time-series analysts.&nbsp;Validity of the models has been substantiated by the&nbsp;overwhelming success witnessed by three decades of&nbsp;time-series analysis using these models.</span></p>
<p><span class="font3">To obtain probabilities for the </span><span class="font7">CARS ALES </span><span class="font3">model, we first assess from an expert the functional form of the&nbsp;conditional probabilities. The expert may be able to&nbsp;specify qualitatively the form of the conditional probabilities by choosing to focus on the key determinants&nbsp;of the distribution that are known to be time invariant. For example, the supply of U.S. manufactured&nbsp;cars will decrease invariably in direct proportion to&nbsp;the Japanese demand. Although last months manufacture price and the U.S. car industry health will&nbsp;modulate the absolute effect, the </span><span class="font8" style="font-style:italic;">supply</span><span class="font7"> </span><span class="font3">node^ conditional probability should still reflect the putative relationship between supply and demand.</span></p>
<p><span class="font3">Once the appropriate parameterization has been defined, the conditional probabilities can be refined to reflect the evidence at hand. Such refinement is achieved either through the estimation of parameter values from&nbsp;data using maximum-likelihood methods or through&nbsp;user specification. In either case, the advantage of this&nbsp;approach is a parsimonious and adaptive representation of the conditional probabilities.</span></p>
<p><span class="font8">3.2 &nbsp;&nbsp;&nbsp;Updating the Model with Data</span></p>
<p><span class="font3">In time-series analyses, dynamic models adapt to changes in system behavior through the reestimation&nbsp;of model parameters when new observations are made.&nbsp;The process of </span><span class="font8" style="font-style:italic;">updating</span><span class="font7" style="font-variant:small-caps;"> carsales </span><span class="font3">is the iterative process by which new observations are used to update&nbsp;the maximum-likelihood estimates of the likelihood&nbsp;weighting coefficients in Equation 1. A </span><span class="font8" style="font-style:italic;">model update of&nbsp;</span><span class="font7">CARSALES </span><span class="font3">is an adaptive learning procedure in which&nbsp;the underlying model is changed; it should be distinguished from a </span><span class="font8" style="font-style:italic;">belief update.</span><span class="font7"> </span><span class="font3">In the model update,&nbsp;we use new evidence to update our prior belief in the&nbsp;model specification; in contrast, in a belief update, new&nbsp;evidence changes our belief in a proposition inferred&nbsp;from the belief network. Both methods use maximum&nbsp;likelihood to update the respective beliefs. Once the&nbsp;parameters are updated, the conditional-probability&nbsp;distributions of the model are reevaluated.</span></p>
<p><span class="font8">3.3 &nbsp;&nbsp;&nbsp;Beyond Two-State Models</span></p>
<p><span class="font3">The specification of the </span><span class="font7">CARSALES </span><span class="font3">DNM tacitly </span><span class="font7">as</span><span class="font3">sumes that maintaining observations from two distinct times is sufficient to provide reliable forecasts. This&nbsp;simplistic model is intended only to highlight the essential features of DNMs. More realistic models for&nbsp;forecasting car sales in Japan might include an array&nbsp;of domain variables and dependencies intended to capture important exogenous influences. Included among&nbsp;these dependencies would be </span><span class="font8" style="font-style:italic;">lag</span><span class="font7"> </span><span class="font3">dependencies—that&nbsp;is, dependencies between nonadjacent time points.&nbsp;These dependencies can model the lagged effects of a&nbsp;policy; for example, federal tax breaks for the car industry would be expected to have an effect on industry&nbsp;health only in the subsequent fiscal year. Lag dependencies are critical in modeling the cyclical behavior&nbsp;of domain variables. The Japanese demand can be&nbsp;expected to display seasonality—that is, it should follow closely the seasonal behavior of unemployment. If&nbsp;we are interested in the seasonal behavior of Japanese&nbsp;demand, we would structure </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">differently.</span></p>
<p><span class="font8">3.4 Diagnostic Checks</span></p>
<p><span class="font3">If the intent of building a DNM is to provide a means for making optimal forecasts of future values of endogenous variables, observed values should be in the&nbsp;immediate neighborhood of predicted values. The&nbsp;</span><span class="font8" style="font-style:italic;">residuals</span><span class="font7"> </span><span class="font3">of a DNM at time </span><span class="font8" style="font-style:italic;">t</span><span class="font7"> </span><span class="font3">denote the difference&nbsp;between the one-step-ahead forecasts and the values&nbsp;observed at time </span><span class="font8" style="font-style:italic;">t.</span><span class="font7"> </span><span class="font3">If the DNM is correctly assessed&nbsp;by the expert, the residuals should be distributed normally and independently with mean zero. A plot of&nbsp;the residual sample autocorrelations should not reveal&nbsp;the presence of serial correlation. A serial correlation&nbsp;indicates that there is some systematic aspect of the&nbsp;behavior of the system that is not being detected by&nbsp;the model. Thus, diagnostic checks assess the adequacy of the model and can serve to suggest appropriate modifications.</span></p><h3><a name="bookmark6"></a><span class="font10" style="font-weight:bold;">4 FORECASTING: INFERENCE&nbsp;WITH A DNM</span></h3>
<p><span class="font3">Ongoing implementations of complex DNMs include a sleep apnea model and a model for surgical intensive-care ventilator management. Validation of the performance of these models is underway. Here, we&nbsp;shall highlight key phases of updating and forecasting with DNMs with analytical results derived from&nbsp;the </span><span class="font7">CARSALES </span><span class="font3">DNM.</span></p>
<p><span class="font3">To forecast at time </span><span class="font8" style="font-style:italic;">t</span><span class="font7"> </span><span class="font3">the ^ + 1 values of the nodes in </span><span class="font7" style="font-variant:small-caps;">carsales, </span><span class="font3">we </span><span class="font8" style="font-style:italic;">scroll</span><span class="font7"> </span><span class="font3">the model one time slice into the&nbsp;future. Figure 2 depicts the process of scrolling the&nbsp;model. The scrolled model uses the same values estimated at time </span><span class="font8" style="font-style:italic;">t</span><span class="font3"> for the likelihood weights combining&nbsp;the contemporaneous and the noncontemporaneous influences on supply. Thus, the model implicitly reflects&nbsp;the effects of unmodeled exogenous forces on the level&nbsp;of the forecasts, thus, increasing the forecast reliability. In general, to project </span><span class="font8" style="font-style:italic;">l</span><span class="font3"> time points into the future,&nbsp;a DNM at time </span><span class="font8" style="font-style:italic;">t</span><span class="font7"> </span><span class="font3">sequentially steps through the time&nbsp;points </span><span class="font7">&lt; + </span><span class="font3">1,</span><span class="font7">.... </span><span class="font8" style="font-style:italic;">t + l.</span><span class="font7"> </span><span class="font3">Thus, a complete profile of the&nbsp;time series over the time interval from &lt; to &lt; + / is&nbsp;constructed in the process of forecasting.</span></p>
<p><span class="font3">Once the projection model at time </span><span class="font8" style="font-style:italic;">t+l</span><span class="font7"> </span><span class="font3">has been identified, the forecasted values of the endogenous variables at time </span><span class="font8" style="font-style:italic;">t + l</span><span class="font7"> </span><span class="font3">are computed by a probabilistic inference</span></p>
<p><span class="font3">1-2 &nbsp;&nbsp;&nbsp;i-l&nbsp;&nbsp;&nbsp;&nbsp;&lt;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">t+l</span></p><img src="DBN_files/DBN-2.jpg" style="width:207pt;height:74pt;"/>
<p><span class="font15" style="font-style:italic;">1-2</span><span class="font15"> &nbsp;&nbsp;&nbsp;(-3&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font15" style="font-style:italic;">t+i</span></p><img src="DBN_files/DBN-3.jpg" style="width:204pt;height:73pt;"/>
<p><span class="font3">Figure 2: Forecasting with </span><span class="font7" style="font-variant:small-caps;">carsales. </span><span class="font3">In going from figure (a) to figure (b), the model is scrolled on time&nbsp;slice into the future. The forecasted values at time&nbsp;&lt; + 1 are computed from the model in figure (b), while&nbsp;preserving the likelihood weights computed in (a) and&nbsp;the set of observations made in (a) for nodes at time</span></p>
<p><span class="font3">algorithm. For typically complex applications, the size and topology of the DNM may prohibit tractable exact&nbsp;computation of inferences. In such cases, stochastic-simulation algorithms designed specifically to approximate inference probabilities in large belief networks&nbsp;can be employed [15, 14, 11, 16, 9, 17, 4, 6, 7].</span></p><h3><a name="bookmark7"></a><span class="font10" style="font-weight:bold;">5 SPECIAL DNMS</span></h3>
<p><span class="font3">We can simplify the assessment of conditional probabilities for DNMs by employing special parameterized functional forms. We shall focus on two simple parametric decompositions used commonly by time-series&nbsp;analysts: the </span><span class="font8" style="font-style:italic;">additive</span><span class="font7"> </span><span class="font3">and the </span><span class="font8" style="font-style:italic;">multiplicative</span><span class="font7"> </span><span class="font3">decompositions. The additive decomposition is used commonly in time-series analysis for integrating predictions based on current observations with predictions&nbsp;based on historical observations. Additive decompositions are an integral aspect of models that purport to&nbsp;forecast future values of time series, and they appear&nbsp;in the form of the Kalman filter in state-space models, and in the conditional sum of squares in ARIMA&nbsp;models [10]. The multiplicative decomposition is used&nbsp;commonly to model log-linear systems in engineering&nbsp;applications.</span></p>
<p><span class="font3">Both decompositions employ </span><span class="font8" style="font-style:italic;">likelihood weights,</span><span class="font7"> </span><span class="font3">which provide a language for assigning measures of reliability to information about different times. With this&nbsp;approach, we consider the probabilistic dependencies&nbsp;from contemporaneous sets of variables and from variables at different points in the past as providing independent sources of information. These measures are&nbsp;used to weight the contribution of the contemporaneous and noncontemporaneous dependencies differently.&nbsp;The sum of the predictions, each weighted by its likelihood, gives the final prediction. The use of likelihood&nbsp;weighting allows an expert to specify the weight of the&nbsp;past versus the present with ease. Such parameters&nbsp;in the functional forms also allow the distributions to&nbsp;be tuned adaptively as dictated by a set of current&nbsp;observations.</span></p>
<p><span class="font3">In the </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">model, the supply at time </span><span class="font8" style="font-style:italic;">t<sub>y</span><span class="font3"></sub> based on information at time depends on the demand and&nbsp;industry health at time denoted by </span><span class="font8" style="font-style:italic;">Q[st\dt,ht).</span><span class="font3"> The&nbsp;supply at time based on information prior to time&nbsp;</span><span class="font20">艺</span><span class="font3">，depends on the price and supply at time </span><span class="font20">艺一</span><span class="font3">1</span><span class="font20">，</span><span class="font3"> de</span><span class="font20">尋&nbsp;</span><span class="font3">noted by&nbsp;&nbsp;&nbsp;&nbsp;Let </span><span class="font8" style="font-style:italic;">a</span><span class="font3"> denote the likelihood</span></p>
<p><span class="font3">that the supply predicted from the information prior to time </span><span class="font8" style="font-style:italic;">t</span><span class="font3"> is correct; therefore, 1 — a denotes the likelihood that the supply predicted from information at&nbsp;time </span><span class="font8" style="font-style:italic;">t</span><span class="font3"> is correct. In the additive decomposition, the&nbsp;prediction of supply is given by</span></p>
<p><span class="font3" style="font-style:italic;">?r[s<sub>t</sub> Idt.ht.pt-uSt^^a]</span><span class="font5"> = </span><span class="font3" style="font-style:italic;">aQ[s<sub>t</sub>\d</span><span class="font5"><sub>t</sub>, </span><span class="font3" style="font-style:italic;">h<sub>t</sub>)</span></p>
<p><span class="font12">+ </span><span class="font18">(1</span><span class="font12"> —</span></p>
<p><span class="font3">In the multiplicative decomposition, the prediction is</span></p>
<p><span class="font3">= </span><span class="font8" style="font-style:italic;">XQ[s<sub>t</sub>\d<sub>t</sub>,h<sub>t</sub>]<sup>a</sup></span></p>
<p><span class="font3">x </span><span class="font8" style="font-style:italic;">R[s<sub>t</sub>\pt-i,s<sub>t</sub>-iY~<sup>a</sup></span></p>
<p><span class="font3">where ^ is a constant that normalizes the probability to unity.</span></p>
<p><span class="font3">We proceed to study the adaptive behavior of the additive decomposition given new evidence. We focus on the behavior of the </span><span class="font7">CARSALES </span><span class="font3">model. Let&nbsp;5,-, </span><span class="font8" style="font-style:italic;">i &lt; t,</span><span class="font3"> denote the values of the supply nodes&nbsp;observed for time points up to, and including, time&nbsp;point and similarly, let </span><span class="font8" style="font-style:italic;">i &lt; t<sub>y</span><span class="font3"></sub> denote the set&nbsp;of values observed for nodes </span><span class="font8" style="font-style:italic;">di<sub>y</sub>hi^pi.</span><span class="font3"> We are interested in evaluating the conditional likelihood function&nbsp;&nbsp;&nbsp;&nbsp;<sup>s</sup>&lt;-</span><span class="font0">2</span><span class="font3">,Pi-</span><span class="font0">2</span><span class="font3">；^], predicted by our</span></p>
<p><span class="font3">model </span><span class="font7">CARSALES, </span><span class="font3">of observing &nbsp;&nbsp;&nbsp;and </span><span class="font8" style="font-style:italic;">s%.</span><span class="font3"> From</span></p>
<p><span class="font3">Equation 1 the conditional probability for the supply node depends on the chosen value of the likelihood&nbsp;weight </span><span class="font1">or, </span><span class="font3">and we expect </span><span class="font8" style="font-style:italic;">L</span><span class="font3"> to vary accordingly. If </span><span class="font8" style="font-style:italic;">a*&nbsp;</span><span class="font3">ma-ximizes </span><span class="font8" style="font-style:italic;">L<sub>y</span><span class="font3"></sub> then </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> is the maximum-likelihood estimator of a, and it is optimal in the sense that it is&nbsp;an unbiased estimator of </span><span class="font8" style="font-style:italic;">a</span><span class="font3"> that achieves the Cramer-Rao lower bound on the variance for all possible unbiased estimators. Accordingly, predictions made by&nbsp;</span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">using maximum-likelihood estimators for&nbsp;the parameters are optimal over the space of all possible unbiased estimators.</span></p>
<p><span class="font3">We begin by giving the expression for </span><span class="font8" style="font-style:italic;">L</span><span class="font3"> in the </span><span class="font7" style="font-variant:small-caps;">carsales example,</span></p>
<p><span class="font8" style="font-style:italic;">L =</span><span class="font3"> Pr[s<sub>t</sub>|&lt;f<sub>t</sub>,/i(,p<sub>t)</sub>s<sub>t</sub>_i</span><span class="font20">；</span><span class="font3">a]</span></p>
<p><span class="font3">If we substitute for each conditional probability in the preceding expression for </span><span class="font8" style="font-style:italic;">L</span><span class="font3"> the expression given by&nbsp;the additive decomposition, the resulting equation is</span></p>
<p><span class="font3">a quadratic equation in </span><span class="font8" style="font-style:italic;">a.</span></p>
<p><span class="font8" style="font-style:italic;">L</span><span class="font3"> — </span><span class="font20">〇</span><span class="font3">f </span><span class="font8" style="font-style:italic;">Sf</span><span class="font3"> i</span></p>
<p><span class="font7">+ </span><span class="font13" style="font-style:italic;">OC</span><span class="font7"> [&lt;5t &nbsp;&nbsp;&nbsp;1 </span><span class="font13" style="font-style:italic;">\s<sub>t</sub>^2,Pt-2]</span><span class="font7"> +&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font13" style="font-style:italic;">Pt-i]]</span></p>
<p><span class="font16">+ (</span><span class="font17">1</span><span class="font16">)</span></p>
<p><span class="font3">where, for </span><span class="font8" style="font-style:italic;">i — t —</span><span class="font3"> i〆</span><span class="font20">，</span></p>
<p><span class="font7"><sup>=</sup> &nbsp;&nbsp;&nbsp;|— </span><span class="font2">1</span><span class="font7"> &gt; P* — </span><span class="font2">1</span><span class="font7"> ]•&nbsp;&nbsp;&nbsp;&nbsp;(</span><span class="font2">2</span><span class="font7">)</span></p>
<p><span class="font3">Equation 1 is the equation of a parabola with extremum occurring at</span></p>
<p><span class="font22">〜二 &nbsp;&nbsp;&nbsp;</span><span class="font17" style="font-style:italic;">26<sub>t</sub>6t</span><span class="font14" style="font-style:italic;"> — </span><span class="font17" style="font-style:italic;">1</span></p><a name="caption2"></a><h1><a name="bookmark8"></a><span class="font17">. ... </span><span class="font4">(<sup>3</sup>)</span></h1>
<p><span class="font3">The maximum-likelihood estimator a* must lie in the</span></p>
<p><span class="font3">interval [0,1]. The constraint leads to the following</span></p>
<p><span class="font3">two cases; (1) if &nbsp;&nbsp;&nbsp;&gt; 0, then the parabola is</span></p>
<p><span class="font3">convex-up, and thus, a* = 0 if </span><span class="font8" style="font-style:italic;">a<sub>m</sub> &lt;</span><span class="font3"> | and </span><span class="font8" style="font-style:italic;">a* = 1</span></p>
<p><span class="font3">if </span><span class="font8" style="font-style:italic;">a<sub>m</sub> &gt;</span><span class="font3"> &nbsp;&nbsp;&nbsp;(2)&nbsp;&nbsp;&nbsp;&nbsp;&lt; 0, and, either </span><span class="font8" style="font-style:italic;">a<sub>m</sub> &lt;</span><span class="font3"> 0 and</span></p>
<p><span class="font3">cv* = </span><span class="font20">〇</span><span class="font3">, </span><span class="font20">〇</span><span class="font3">r </span><span class="font20">〇</span><span class="font3"> &lt; &nbsp;&nbsp;&nbsp;$ 1 and ct* = or a<sub>m</sub> &gt; 1 and</span></p>
<p><span class="font8" style="font-style:italic;">a* — l.</span></p>
<p><span class="font3">In terms of the model structure, a choice </span><span class="font8" style="font-style:italic;">of a* = l </span><span class="font3">implies that predictions based on prior information&nbsp;should be ignored categorically. This conclusion is&nbsp;consistent with the finding that, if&nbsp;&nbsp;&nbsp;&nbsp;&gt; 0, for</span></p>
<p><span class="font8" style="font-style:italic;">i</span><span class="font3"> = </span><span class="font8" style="font-style:italic;">t —</span><span class="font3"> l,t, the values of </span><span class="font8" style="font-style:italic;">d{</span><span class="font3"> and </span><span class="font8" style="font-style:italic;">hi</span><span class="font3"> are correlated more strongly with the outcome </span><span class="font8" style="font-style:italic;">$i</span><span class="font3"> than are the values of 5,_i and Pt_i. The conclusion follows because&nbsp;</span><span class="font8" style="font-style:italic;">prior</span><span class="font3"> to observing the outcome for supply at time </span><span class="font8" style="font-style:italic;">i<sub>1&nbsp;</span><span class="font3"></sub>the probabilities Q[5j|di,/ij] and&nbsp;&nbsp;&nbsp;&nbsp;are</span></p>
<p><span class="font3">distributions for the node </span><span class="font8" style="font-style:italic;">Si</span><span class="font3"> predicted from current information, </span><span class="font8" style="font-style:italic;">di</span><span class="font3"> and and from prior information,&nbsp;</span><span class="font8" style="font-style:italic;">Si</span><span class="font6" style="font-style:italic;">-1</span><span class="font3"> and Pi-i. Thus, these probabilities are a measure&nbsp;of the correlation of the current and the prior information with the finding—that is, the observed outcome&nbsp;</span><span class="font8" style="font-style:italic;">Si</span><span class="font19" style="font-style:italic;">，</span><span class="font3">Conversely, a choice of a* = 0 implies that predictions based on current information should be categorically ignored. When 6t-i and </span><span class="font8" style="font-style:italic;">6t</span><span class="font3"> are of opposite signs,&nbsp;the appropriate choice of </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> may take on a value intermediate between 0 and 1, implying that the prediction&nbsp;for supply based on maximum-likelihood is a weighted&nbsp;mix of the two predictions.</span></p>
<p><span class="font3">Once </span><span class="font8" style="font-style:italic;">a</span><span class="font3"> has been computed—that is, updated to reflect new observations—the model can be used to forecast&nbsp;next month’s </span><span class="font7">supply. </span><span class="font3">We scroll </span><span class="font7">CARSALES </span><span class="font3">forward one&nbsp;time point, and compute the marginalized distribution&nbsp;Pr[5&lt;+i|^] in the belief network using probabilistic inference. For large models, or for models used in high&nbsp;stakes time-pressured environments, an approximate&nbsp;algorithm that trades off accuracy of inference for efficiency of computation is preferred.</span></p><h3><a name="bookmark9"></a><span class="font10" style="font-weight:bold;">6 NUMERICAL EXAMPLE</span></h3>
<p><span class="font3">Let us consider an example using the additive model for </span><span class="font7" style="font-variant:small-caps;">carsales. </span><span class="font3">We assume that all nodes are binary—&nbsp;that is, </span><span class="font8" style="font-style:italic;">demand, price, industry health</span><span class="font3"> and </span><span class="font8" style="font-style:italic;">sales</span><span class="font3"> are&nbsp;all either </span><span class="font8" style="font-style:italic;">high</span><span class="font3"> (H) or </span><span class="font8" style="font-style:italic;">low</span><span class="font3"> (L). The conditional probability distributions for </span><span class="font8" style="font-style:italic;">Q[s<sub>t</span><span class="font3"></sub> = </span><span class="font8" style="font-style:italic;">H\d<sub>t}</sub>h<sub>t</sub>]</span><span class="font3"> and </span><span class="font8" style="font-style:italic;">R[s<sub>t</span><span class="font3"></sub> =</span></p>
<table border="1">
<tr><td>
<p><span class="font8" style="font-style:italic;">d，i hf</span></p></td><td>
<p><span class="font3">= </span><span class="font8" style="font-style:italic;">H\d<sub>tl</sub>h<sub>t</sub>]</span></p></td></tr>
<tr><td>
<p><span class="font3">HH</span></p></td><td>
<p><span class="font3">0.55</span></p></td></tr>
<tr><td>
<p><span class="font3">HL</span></p></td><td>
<p><span class="font3">0.25</span></p></td></tr>
<tr><td>
<p><span class="font3">LH</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.60</span></p></td></tr>
<tr><td>
<p><span class="font3">LL</span></p></td><td>
<p><span class="font3">0.55</span></p></td></tr>
</table>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font3">W</span><span class="font20">卜二</span><span class="font3">//</span><span class="font20">丨外一</span><span class="font3">^</span><span class="font20">一」</span></p></td></tr>
<tr><td>
<p><span class="font3">HH</span></p></td><td>
<p><span class="font3">0.90</span></p></td></tr>
<tr><td>
<p><span class="font3">HL</span></p></td><td>
<p><span class="font3">0.40</span></p></td></tr>
<tr><td>
<p><span class="font3">LH</span></p></td><td>
<p><span class="font3">0.40</span></p></td></tr>
<tr><td>
<p><span class="font3">LL</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.10</span></p></td></tr>
</table>
<p><span class="font3">Figure</span></p>
<p><span class="font3">3: The conditional probabilities Q[5t = </span><span class="font8" style="font-style:italic;">H\d<sub>ti</sub>p<sub>t</sub>]</span><span class="font3"> and </span><span class="font8" style="font-style:italic;">R[s<sub>t</span><span class="font3"></sub> =&nbsp;&nbsp;&nbsp;&nbsp;in </span><span class="font7">CARSALES. </span><span class="font3">In the example, the</span></p>
<p><span class="font3">conditional probability for the car-sales node is given by the convex combination of the probabilities </span><span class="font8" style="font-style:italic;">Q</span><span class="font3"> and&nbsp;</span><span class="font8" style="font-style:italic;">R</span><span class="font3"> with parameter </span><span class="font8" style="font-style:italic;">a.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font8" style="font-style:italic;">Pt</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">Prld<sub>t</sub> = </span><span class="font8" style="font-style:italic;">H\p<sub>t</sub>\</span></p></td></tr>
<tr><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">0.25</span></p></td></tr>
<tr><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">0.65</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font8" style="font-style:italic;">h<sub>t</sub></span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-style:italic;">Pr\p<sub>t</span><span class="font3"></sub> = </span><span class="font8" style="font-style:italic;">H\h<sub>t</sub>]</span></p></td></tr>
<tr><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">0,35</span></p></td></tr>
<tr><td>
<p><span class="font3">L</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
</table>
<p><span class="font3">Figure 4: The conditional probabilities Pr[d<sub>f</sub> =</span><span class="font8" style="font-style:italic;">H\Pt] </span><span class="font3">and Pr[p<sub>f</sub> = </span><span class="font8" style="font-style:italic;">H\ht]</span><span class="font3"> </span><span class="font7">of carsales. </span><span class="font3">The prior probability&nbsp;for the industry health is Pr[A&lt; </span><span class="font8" style="font-style:italic;">= H] =</span><span class="font3"> 0.85.</span></p>
<p><span class="font3">//|5j_i ,pc_i] are given in Figure 3. The remaining conditional probabilities for </span><span class="font7">CARSALES </span><span class="font3">are given in&nbsp;Figure 4.</span></p>
<p><span class="font3">Figure 5 contains a time-series of observations of the variables of </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">made over twelve consecutive&nbsp;periods. Initially, the time-series is assumed to be stationary with data reflecting a healthy U.S. car industry&nbsp;and high-volume sales in Japan. The model is in equilibrium with the time-series, where in this example,&nbsp;the equilibrium is reflected in the value of which&nbsp;takes on the value 0 when equilibrium is reached. Exogenous events that disturb the level of the time-series&nbsp;occur at ^ = 3,6,9, </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">adapts to the exogenous disturbance with a* increasing and subsequently&nbsp;decreasing over ensuing periods. The number of periods over which </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> increases is a function of the lagged&nbsp;influences in the model—that is, the noncontemporaneous relations.</span></p>
<p><span class="font3">At f = 3 we observe a sudden drop in the demand which persists through subsequent time periods. The&nbsp;persistence of this drop allows us to rule out noise as&nbsp;the cause, and we can assume that the change in the&nbsp;demand has been precipitated by an exogenous event.</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font3">t</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">6</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">d<sub>t</sub></span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">h<sub>t</sub></span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">Pt</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">St</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">a*</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0</span></p></td><td>
<p><span class="font3">L0</span></p></td><td>
<p><span class="font3">0.5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0,0</span></p></td></tr>
<tr><td>
<p><span class="font3"><sup>s</sup>t+i</span></p></td><td>
<p></p></td><td>
<p><span class="font3">0.40</span></p></td><td>
<p><span class="font3">0.40</span></p></td><td>
<p><span class="font3">0.56</span></p></td><td>
<p><span class="font3">0.73</span></p></td><td>
<p><span class="font3">0.90</span></p></td><td>
<p><span class="font3">0.90</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font3">t</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">10</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">11</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">d<sub>t</sub></span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">h<sub>t</sub></span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p><span class="font8" style="font-style:italic;">Pt</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">H</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td><td>
<p><span class="font3">L</span></p></td></tr>
<tr><td>
<p><span class="font3">a*</span></p></td><td>
<p><span class="font3">0.5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.0</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.0</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font3">0.48</span></p></td><td>
<p><span class="font3">0.56</span></p></td><td>
<p><span class="font3">0.56</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.10</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.10</span></p></td></tr>
</table>
<p><span class="font3">Figure 5: Time-series of data for </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">over twelve periods. Values for </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> are maximum-likelihood estimates corresponding to the observed data. For each&nbsp;period, we forecast the probability that the supply will&nbsp;be high in the next period. We denote this forecast by&nbsp;?<sub>t+1</sub>, and we compute it by evaluating the inference&nbsp;</span><span class="font7">Pr[^+i = in carsales </span><span class="font3">with the a* computed&nbsp;at time </span><span class="font8" style="font-style:italic;">t</span><span class="font3"> and where denotes all observations made&nbsp;up to and including time </span><span class="font8" style="font-style:italic;">t.</span></p>
<p><span class="font3">Concomitant with the low demand there is an increase in supply, while both industry health and price remain&nbsp;temporarily unchanged, </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">responds to the&nbsp;disturbance by temporarily increasing </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> until a new&nbsp;equilibrium is achieved. The forecasted probabilities&nbsp;for a high supply rise from 0.4 to 0.9 during this period.&nbsp;At ^ = 6, we observe a drop in price which once again&nbsp;disturbs the equilibrium state. The low price continues through remaining periods, and reflects, to some&nbsp;extent, the market forces that result form high supply&nbsp;and low demand which are captured by the supply-demand model </span><span class="font7" style="font-variant:small-caps;">carsales. </span><span class="font3">However, the probability&nbsp;that the price drops in </span><span class="font7" style="font-variant:small-caps;">carsales </span><span class="font3">when demand is low&nbsp;and supply and industry health are high, cannot alone&nbsp;account for the persistence of the low price. We anticipate that the price has been exogenously clamped at&nbsp;a low value. Although a single drop in price at t = 6&nbsp;does not affect the value of </span><span class="font8" style="font-style:italic;">a</span><span class="font3">*, the consistently low values represent an unlikely circumstance. The </span><span class="font7" style="font-variant:small-caps;">carsales&nbsp;</span><span class="font3">model responds to this scenario by adjusting </span><span class="font8" style="font-style:italic;">a*</span><span class="font3"> during the subsequent two time periods. The forecasted&nbsp;probability for supply made in period 7 drops in anticipation of a drop in supply by period 8, but which&nbsp;is first observed at period 9. At ^ = 9, subject to declining prices and demand, the health of the industry&nbsp;deteriorates, and consequently, supply drops as well.&nbsp;Once again, the disturbance is sensed by </span><span class="font7" style="font-variant:small-caps;">carsales&nbsp;</span><span class="font3">through the parameter a*, and the forecasts for supply&nbsp;drop even further. In the absence of further changes,&nbsp;by period 11 the model is again at equilibrium.</span></p><h3><a name="bookmark10"></a><span class="font10" style="font-weight:bold;">7 &nbsp;&nbsp;&nbsp;RELATED WORK</span></h3>
<p><span class="font3">Most research in temporal reasoning has focused on the representation of time with logical predicates,&nbsp;rather than on dynamic modeling of the state of the&nbsp;world under uncertainty [2,13,18]. Several approaches&nbsp;have been proposed to support temporal reasoning&nbsp;using probability theory. To date, most of this research </span><span class="font8" style="font-style:italic;">has</span><span class="font3"> been hindered by problems with capturing dynamic changes in temporal models as new ob-ser vat ions become available. Early attempts to develop probabilistic methods for temporal reasoning&nbsp;have posed static models of dynamic domains, in which&nbsp;exogenous influences are captured in fixed conditional-probability distributions. Such models rely entirely on&nbsp;prior knowledge of the domain, and offer no method&nbsp;for refining the probabilistic dependencies of the models dynamically with new data. DNMs can adapt to&nbsp;exogenous influences by fine tuning their conditional-probability distributions. Such dynamic adaptation&nbsp;can reduce forecasting errors and improve planning&nbsp;and control.</span></p>
<p><span class="font3">Cooper, et al. [5] propose methods for encoding uncertain temporal relationships under several restrictive assumptions, to allow the exact computation of the joint probability of a hypothesis and the accumulated temporal evidence. Dean and Kanazawa [8] develop a probabilistic model for projection based on a&nbsp;functional (e.g., exponential) decay model of the persistence with time of propositions. Berzuini [3] embeds semi-Markov models in a belief-network representation and uses approximate probabilistic inference&nbsp;to compute the degree of belief in past states and in&nbsp;future states. Tat man shows that a Markov decision&nbsp;process can be encoded in an influence diagram [19].&nbsp;Kanazawa and Dean [12] apply approximate decisionmaking processes to these influence diagrams to trade&nbsp;off accuracy of prediction for speed of decision making.</span></p>
<p><span class="font3">Most of the models developed in research on temporal reasoning have a limited ability to adapt to new observations, and their Markov nature, prevents them from&nbsp;making forecasts that extend beyond Markov simulations. Recent work by Abramson [1] describes a belief-network model that predicts future crude-oil prices&nbsp;given historical evidence. However, he employs classical time-series methods external to the belief-network&nbsp;to reestimate model parameters.</span></p><h3><a name="bookmark11"></a><span class="font10" style="font-weight:bold;">8 &nbsp;&nbsp;&nbsp;CONCLUSION</span></h3>
<p><span class="font3">The temporal dependencies and time-based evolution of the states of variables in a dynamic domain limit&nbsp;the applicability of conventional static belief-network&nbsp;models. DNMs extend classical dynamic modeling,&nbsp;by providing an expressive language and platform for&nbsp;ongoing research on probabilistic temporal reasoning.&nbsp;DNMs are ideally suited for forecasting and control in&nbsp;domains for which detailed prior knowledge is available about the dynamic forces and relations at play,&nbsp;but is sufficiently complex to preclude a complete&nbsp;specification. These are the domains that we face in&nbsp;probabilistic-reasoning applications. With the DNM&nbsp;approach, machinery is provided for compensating for&nbsp;the exogenous influences in an unbiased fashion. Our&nbsp;future research on DNMs includes exploring the performance of alternate inference algorithms to solve special DNM topologies, validating the predictive behavior of alternative models, and investigating methods&nbsp;for inducing DNMs from static belief networks by identifying temporal dependencies from time-series data.</span></p><h3><a name="bookmark12"></a><span class="font10" style="font-weight:bold;">Acknowledgments</span></h3>
<p><span class="font3">This work was supported by the National Science Foundation under grant IRI-9108385 and Rockwell&nbsp;Science Center IR&amp;D funds.</span></p>
<p><span class="font9" style="font-weight:bold;">References</span></p>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;Bruce Abramson. ARCOl: An application of belief networks to the oil market. In </span><span class="font8" style="font-style:italic;">Proceedings&nbsp;of the Seventh Conference on Uncertainty in Artificial Intelligence^</span><span class="font3"> pages 1-8, Los Angeles, CA,&nbsp;July 1991. Association for Uncertainty in Artificial Intelligence.</span></p>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;J. Allen. Towards a general theory of action and&nbsp;time. </span><span class="font8" style="font-style:italic;">Journal of Computer and Systems Sciences<sub>y&nbsp;</span><span class="font3"></sub>31(2):288-301, 1985.</span></p>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;C. Berzuini, R. Bellcizzi, and S. Quaglini, Temporal reasoning with probabilities. In </span><span class="font8" style="font-style:italic;">Proceedings&nbsp;of the 1989 Workshop on Uncertainty in Artificial&nbsp;Intelligence^</span><span class="font3"> pages 14-21, Windsor, Ontario, July</span></p>
<p><span class="font3">1989. &nbsp;&nbsp;&nbsp;Association for Uncertainty in Artificial Intelligence.</span></p>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;R. Chavez and G. Cooper. A randomized approximation algorithm for probabilistic inference on&nbsp;bayesian belief networks. </span><span class="font8" style="font-style:italic;">Networks^</span><span class="font3"> 20:661-685,</span></p>
<p><span class="font3">1990.</span></p>
<p><span class="font3">[5] G. Cooper, E. Horvitz, and D. Heckerman. A method for temporal probabilistic reasoning.&nbsp;Technical Report KSL 88-30, Medical Computer&nbsp;Science, Stanford University, Stanford, CA, August 1988.</span></p>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;P. Dagum and R.M. Chavez. Approximating&nbsp;probabilistic inference in bayesian belief networks.&nbsp;Technical Report KSL-91-46, Knowledge Systems&nbsp;Laboratory, Stanford University, Stanford, CA,&nbsp;July 1991.</span></p>
<p><span class="font3">[7] P. Dagum and E. Horvitz. &nbsp;&nbsp;&nbsp;An analysis of&nbsp;Monte-Carlo algorithms for probabilistic inference. Technical Report KSL-91-67, Knowledge&nbsp;Systems Laboratory, Stanford University, Stanford, CA, October 1991.</span></p>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;T. Dean and K. Kanazawa. Probabilistic causal&nbsp;reasoning. In </span><span class="font8" style="font-style:italic;">Proceedings of ike Fourth Workshop&nbsp;on Uncertainty in Artificial Inielligence<sub>y</span><span class="font3"></sub> pages</span></p>
<p><span class="font3">73-80, Minneapolis, MN, August 1988. American Association for Artificial Intelligence.</span></p>
<p><span class="font3">[9] R. Fung and K.-C. Chang. Weighing and integrating evidence for stochastic simulation in&nbsp;Bayesian networks. In </span><span class="font8" style="font-style:italic;">Uncertainty in Artificial&nbsp;Intelligence</span><span class="font3"> 5, pages 209-219. Elsevier, Amsterdam, The Netherlands, 1990.</span></p>
<p><span class="font3">[10] &nbsp;&nbsp;&nbsp;A.C. Harvey. </span><span class="font8" style="font-style:italic;">Forecasting, structural time series&nbsp;models, and the Kalman filter.</span><span class="font3"> Cambridge University Press, New York, 1990.</span></p>
<p><span class="font3">[11] &nbsp;&nbsp;&nbsp;M. Henrion. Propagating uncertainty in Bayesian&nbsp;networks by probabilistic logic sampling. In </span><span class="font8" style="font-style:italic;">Uncertainty in Artificial Intelligence</span><span class="font3"> 2, pages 149163. North-Holland, Amsterdam, The Netherlands, 1988.</span></p>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;K. Kanazawa and T. Dean. A model for projection and action. In </span><span class="font8" style="font-style:italic;">Proceedings of the Eleventh&nbsp;International Joint Conference on Artificial Intelligence,</span><span class="font3"> pages 985-990, Detroit, MI, August 1989.</span></p>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;P. Ladkin. Time representation: A taxonomy of&nbsp;interval relations. In </span><span class="font8" style="font-style:italic;">Proceedings of the Fifth National Conference on Artificial Intelligence^</span><span class="font3"> pages&nbsp;360-366, Philadelphia, PA, 1986. American Association for Artificial Intelligence.</span></p>
<p><span class="font3">[14] &nbsp;&nbsp;&nbsp;J. Pearl. Addendum: Evidential reasoning using&nbsp;stochastic simulation of causal models. </span><span class="font8" style="font-style:italic;">Artificial&nbsp;Intelligence^</span><span class="font3"> 33:131, 1987.</span></p>
<p><span class="font3">[15] &nbsp;&nbsp;&nbsp;J. Pearl. Evidential reasoning using stochastic&nbsp;simulation of causal models. </span><span class="font8" style="font-style:italic;">Artificial Intelli-gence<sub>y</span><span class="font3"></sub> 32:245-257, 1987.</span></p>
<p><span class="font3">[16] &nbsp;&nbsp;&nbsp;R. Shachter and M. Peot. Evidential reasoning&nbsp;using likelihood weighting. 1989. Unpublished&nbsp;manuscript.</span></p>
<p><span class="font3">[17] &nbsp;&nbsp;&nbsp;R. Shachter and M. Peot. Simulation approaches&nbsp;to general probabilistic inference on belief networks. In </span><span class="font8" style="font-style:italic;">Uncertainty in Artificial Intelligence&nbsp;</span><span class="font3">5, pages 221-231. Elsevier, Amsterdam, The&nbsp;Netherlands, 1990.</span></p>
<p><span class="font3">[18] &nbsp;&nbsp;&nbsp;Y. Shoham and N. GoyaL Temporal reasoning&nbsp;in artificial intelligence. In </span><span class="font8" style="font-style:italic;">Exploring Artificial&nbsp;Intelligence.</span><span class="font3"> Morgan Kaufmann, San Mateo, CA,</span></p>
<p><span class="font3">1988.</span></p>
<p><span class="font3">[19] &nbsp;&nbsp;&nbsp;J. Tatman. </span><span class="font8" style="font-style:italic;">Decision processes in influence diagrams: Formulation and analysis.</span><span class="font3"> PhD thesis,&nbsp;Engineering-Economic Systems, Stanford University, Stanford, CA, 1985.</span></p>
<p><span class="font3">[20] &nbsp;&nbsp;&nbsp;M. West and J. Harrison. </span><span class="font8" style="font-style:italic;">Bayesian forecasting&nbsp;and dynamic models.</span><span class="font3"> Springer Verlag, New York,</span></p>
<p><span class="font3">1989.</span></p>
<p><a name="bookmark1"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font7">Also at the Palo Alto Laboratory, Rockwell International Science Center, 444 High Street, Palo Alto, California 94301.</span></p>
</body>
</html>